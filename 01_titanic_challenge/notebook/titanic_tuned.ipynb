{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle Titanic Challenge\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the available data files\n",
    "\n",
    "for dirname, dirs, files in os.walk('../data'):\n",
    "    print(f\"Current Directory: {dirname}\")\n",
    "    print(f\"Subdirectories: {dirs}\")\n",
    "    print(f\"Files: {files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do this\n",
    "\n",
    "for dirname, dirs, files in os.walk('../data'):\n",
    "    for filename in files:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the training dataset\n",
    "\n",
    "train_file = '../data/titanic_train.csv'\n",
    "train_df = pd.read_csv(train_file)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the test dataset\n",
    "\n",
    "test_file = '../data/titanic_test.csv'\n",
    "test_df = pd.read_csv(test_file)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the shape of the two datasets\n",
    "\n",
    "print(f\"Training DF: {train_df.shape}\")\n",
    "print(f\"    Test DF: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e21b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at null value stats across the dataset\n",
    "\n",
    "total = train_df.isnull().sum()\n",
    "print(type(total))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify missing data (will call it later...)\n",
    "\n",
    "def missing_data(data):\n",
    "    \"\"\"Custom function to get some stats on missing data.\"\"\"\n",
    "    total = data.isnull().sum()\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
    "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in data.columns:\n",
    "        dtype = str(data[col].dtype)\n",
    "        types.append(dtype)\n",
    "    tt['Types'] = types\n",
    "    return(np.transpose(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0041d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some stats on the features (columns)\n",
    "\n",
    "total = train_df.count()\n",
    "print(total)\n",
    "print(type(total))\n",
    "\n",
    "tt = pd.DataFrame(total)\n",
    "print(tt)\n",
    "print(type(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label for the column (to replace the default numeric index)\n",
    "\n",
    "tt.columns = [\"Total\"]\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfe5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our missing_data function\n",
    "\n",
    "missing_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of women who survived\n",
    "\n",
    "women = train_df.loc[train_df.Sex == 'female']['Survived']\n",
    "rate_women = sum(women) / len(women)\n",
    "print(\"% of women who survived:\", rate_women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of men who survived\n",
    "\n",
    "men = train_df.loc[train_df.Sex == 'male']['Survived']\n",
    "rate_men = sum(men) / len(men)\n",
    "print(\"% of men who survived:\", rate_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0098120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some AI/ML stuff\n",
    "# Building a random forest model\n",
    "\n",
    "y = train_df[\"Survived\"]                        # target variable (did the passenger survive)\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]  # attributes from the input dataset\n",
    "\n",
    "# transform the dataset into a machine learning friendly DataFrame format\n",
    "X = pd.get_dummies(train_df[features])          # features dataframe (after one-hot encoding)\n",
    "X_test = pd.get_dummies(test_df[features])      # features dataframe (after one-hot encoding)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({'PassengerID': test_df.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('../output/submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Improved Random Forest with simple feature engineering & CV ===\n",
    "# This cell leaves your earlier work intact and creates a new submission file.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# --- Safety checks: ensure train_df and test_df exist ---\n",
    "assert \"train_df\" in globals(), \"Expected 'train_df' to be defined earlier in the notebook.\"\n",
    "assert \"test_df\" in globals(), \"Expected 'test_df' to be defined earlier in the notebook.\"\n",
    "\n",
    "# --- Minimal feature engineering ---\n",
    "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Family size & IsAlone\n",
    "    out[\"FamilySize\"] = out[\"SibSp\"].fillna(0) + out[\"Parch\"].fillna(0) + 1\n",
    "    out[\"IsAlone\"] = (out[\"FamilySize\"] == 1).astype(int)\n",
    "    # Name length (a light, often useful signal)\n",
    "    out[\"NameLength\"] = out[\"Name\"].astype(str).str.len()\n",
    "    return out\n",
    "\n",
    "train_fe = add_features(train_df)\n",
    "test_fe  = add_features(test_df)\n",
    "\n",
    "# --- Select columns ---\n",
    "target_col = \"Survived\"\n",
    "numeric_features = [\"Age\", \"Fare\", \"Pclass\", \"SibSp\", \"Parch\", \"FamilySize\", \"IsAlone\", \"NameLength\"]\n",
    "categorical_features = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "X = train_fe[numeric_features + categorical_features]\n",
    "y = train_fe[target_col]\n",
    "X_test = test_fe[numeric_features + categorical_features]\n",
    "\n",
    "# --- Preprocess ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))  # robust to outliers\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # fill missing Embarked/Sex if any\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Tuned RandomForest ---\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,            # let trees grow; RF handles variance with many trees\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[(\"prep\", preprocess), (\"rf\", rf)])\n",
    "\n",
    "# --- Cross-validation to sanity-check improvements ---\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(f\"CV accuracy (mean ± std over 5 folds): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# --- Fit on full training data and predict test ---\n",
    "model.fit(X, y)\n",
    "test_pred = model.predict(X_test).astype(int)\n",
    "\n",
    "# --- Build submission ---\n",
    "assert \"PassengerId\" in test_df.columns, \"Expected 'PassengerId' in test_df for submission.\"\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_df[\"PassengerId\"],\n",
    "    \"Survived\": test_pred\n",
    "})\n",
    "\n",
    "out_csv = \"submission_rf_tuned.csv\"\n",
    "submission.to_csv(out_csv, index=False)\n",
    "print(f\"Submission file written: {out_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
